{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This file is part of Theano Geometry\n",
    "#\n",
    "# Copyright (C) 2017, Stefan Sommer (sommer@di.ku.dk)\n",
    "# https://bitbucket.org/stefansommer/theanogemetry\n",
    "#\n",
    "# Theano Geometry is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# Theano Geometry is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with Theano Geometry. If not, see <http://www.gnu.org/licenses/>.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Simulation of Conditioned Diffusions on Riemannian Manifolds\n",
    "\n",
    "Mathias HÃ¸jgaard Jensen and Stefan Sommer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $S^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.manifolds.S2 import *\n",
    "M = S2(use_spherical_coords=True,chart_center='x')\n",
    "print(M)\n",
    "\n",
    "from src.plotting import *\n",
    "\n",
    "# Riemannian structure\n",
    "from src.Riemannian import metric\n",
    "metric.initialize(M)\n",
    "\n",
    "# geodesics\n",
    "from src.Riemannian import geodesic\n",
    "geodesic.initialize(M)\n",
    "\n",
    "# Logarithm map\n",
    "from src.Riemannian import Log\n",
    "Log.initialize(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brownian motion, coordinate form\n",
    "from src.stochastics import Brownian_coords\n",
    "Brownian_coords.initialize(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delyon/Hu guided process\n",
    "from src.stochastics.guided_process import *\n",
    "\n",
    "# guide function\n",
    "Cholesky = T.slinalg.Cholesky()\n",
    "# phi = lambda q,v: T.tensordot(T.nlinalg.MatrixInverse()(Cholesky(M.gsharp(q))),-(q-v).flatten(),(1,0))\n",
    "phi = lambda q,v: T.tensordot(Cholesky(M.gsharp(q)).T,-(q-v).flatten(),(1,0))\n",
    "x0 = M.element()\n",
    "(Brownian_coords_guided,Brownian_coords_guidedf) = get_guided_likelihood(M,M.sde_Brownian_coords,phi,lambda x: Cholesky(M.gsharp(x)),x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps.set_value(100)\n",
    "\n",
    "x = np.array([np.pi/2,np.pi/2])\n",
    "\n",
    "w = M.Exptf(x,tensor(np.array([.8,-.5])))[-1]\n",
    "(ts,xs,log_likelihood,log_varphi) = Brownian_coords_guidedf(x,w,dWsf(M.dim.eval()))[:4]\n",
    "print(\"log likelihood: \", log_likelihood[-1], \", log varphi: \", log_varphi[-1])\n",
    "\n",
    "# plot\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plotx(xs)\n",
    "M.plotx(x,color='r',s=150)\n",
    "M.plotx(w,color='k',s=150)\n",
    "plt.show()\n",
    "\n",
    "# plot multiple bridges\n",
    "N = 5\n",
    "xss = np.zeros((N,n_steps.eval(),M.dim.eval()),dtype=theano.config.floatX)\n",
    "for i in range(N):\n",
    "    (ts,xs,log_likelihood,log_varphi) = Brownian_coords_guidedf(x,w,.4*dWsf(M.dim.eval()))[:4]\n",
    "    xss[i] = xs\n",
    "\n",
    "# plot\n",
    "M.newfig()\n",
    "M.plot()\n",
    "colormap = plt.get_cmap('winter')\n",
    "colors=[colormap(k) for k in np.linspace(0, 1, N)]\n",
    "for i in range(N):\n",
    "    M.plotx(xss[i],color=colors[i])\n",
    "M.plotx(x,color='r',s=100)\n",
    "M.plotx(w,color='k',s=100)\n",
    "plt.savefig('S2_bridges.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample Data\n",
    "\n",
    "var = .3\n",
    "n_steps.set_value(100)\n",
    "\n",
    "N_samples = 32\n",
    "x = np.array([np.pi/2,np.pi/2])\n",
    "\n",
    "samples = np.zeros((N_samples,M.dim.eval()))\n",
    "for i in range(N_samples):\n",
    "    (ts,xs) = M.Brownian_coordsf(x,var*dWsf(M.dim.eval()))\n",
    "    samples[i] = xs[-1]\n",
    "\n",
    "# plot\n",
    "newfig()\n",
    "M.plot()\n",
    "for i in range(N_samples):\n",
    "    M.plotx(samples[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp guiding code\n",
    "\n",
    "# hit target v at time t=Tend\n",
    "def get_sde_guided(sde_f, phi, sqrtCov, A=None, method='DelyonHu', integration='ito'):\n",
    "    assert (integration is 'ito' or integration is 'stratonovich')\n",
    "    assert (method is 'DelyonHu')  # more general schemes not implemented\n",
    "\n",
    "    def sde_guided(dW, t, x, log_likelihood, log_varphi, h, v, *ys):\n",
    "        (det, sto, X, *dys_sde) = sde_f(dW, t, x, *ys)\n",
    "        h = theano.ifelse.ifelse(T.lt(t, Tend - dt / 2),\n",
    "                                 phi(x, v) / (Tend - t),\n",
    "                                 T.zeros_like(phi(x, v))\n",
    "                                 )\n",
    "        sto = theano.ifelse.ifelse(T.lt(t, Tend - 3 * dt / 2),  # for Ito as well?\n",
    "                                   sto,\n",
    "                                   T.zeros_like(sto)\n",
    "                                   )\n",
    "\n",
    "        ### likelihood\n",
    "        dW_guided = (1 - .5 * dt / (1 - t)) * dW + dt * h  # for Ito as well?\n",
    "        sqrtCovx = sqrtCov(x)\n",
    "        Cov = dt * T.tensordot(sqrtCovx, sqrtCovx, (1, 1))\n",
    "        Pres = T.nlinalg.MatrixInverse()(Cov)\n",
    "        residual = T.tensordot(dW_guided, T.tensordot(Pres, dW_guided, (1, 0)), (0, 0))\n",
    "        log_likelihood = .5 * (-dW.shape[0] * T.log(2 * np.pi) + LogAbsDet()(Pres) - residual)\n",
    "\n",
    "        ## correction factor\n",
    "        ytilde = T.tensordot(X, h * (Tend - t), 1)\n",
    "        tp1 = t + dt\n",
    "        if integration is 'ito':\n",
    "            xtp1 = x + dt * (det + T.tensordot(X, h, 1)) + sto\n",
    "        elif integration is 'stratonovich':\n",
    "            tx = x + sto\n",
    "            xtp1 = x + dt * det + 0.5 * (sto + sde_f(dW, tp1, tx, *ys)[1])\n",
    "        Xtp1 = sde_f(dW, tp1, xtp1, *ys)[2]\n",
    "        ytildetp1 = T.tensordot(Xtp1, phi(xtp1, v), 1)\n",
    "\n",
    "        # set default A if not specified\n",
    "        Af = A if A is not None else lambda x, v, w: T.tensordot(v, T.tensordot(T.nlinalg.MatrixInverse()(T.tensordot(X, X, (1, 1))), w, 1), 1)\n",
    "\n",
    "        #     add t1 term for general phi\n",
    "        #     dxbdxt = theano.gradient.Rop((Gx-x[0]).flatten(),x[0],dx[0]) # use this for general phi\n",
    "        t2 = theano.ifelse.ifelse(T.lt(t, Tend - dt / 2),\n",
    "                                  -Af(x, ytilde, dt * det) / (Tend - t),\n",
    "                                  # check det term for Stratonovich (correction likely missing)\n",
    "                                  constant(0.))\n",
    "        t34 = theano.ifelse.ifelse(T.lt(tp1, Tend - dt / 2),\n",
    "                                   -(Af(xtp1, ytildetp1, ytildetp1) - Af(x, ytildetp1, ytildetp1)) / (\n",
    "                                   2 * (Tend - tp1 + dt * T.gt(tp1, Tend - dt / 2))),\n",
    "                                   # last term in divison is to avoid NaN with non-lazy Theano conditional evaluation\n",
    "                                   constant(0.))\n",
    "        log_varphi = 0 # t2 + t34\n",
    "\n",
    "        return (det + T.tensordot(X, h, 1), sto, X, log_likelihood, log_varphi, dW_guided/dt, T.zeros_like(v), *dys_sde)\n",
    "\n",
    "    return sde_guided\n",
    "\n",
    "def get_guided_likelihood(M, sde_f, phi, sqrtCov, q, A=None, method='DelyonHu', integration='ito'):\n",
    "    sde_guided = get_sde_guided(sde_f, phi, sqrtCov, A, method, integration)\n",
    "    guided = lambda q, v, dWt: integrate_sde(sde_guided,\n",
    "                                             integrator_ito if method is 'ito' else integrator_stratonovich,\n",
    "                                             q, dWt, constant(0.), constant(0.), T.zeros_like(dWt[0]), v)\n",
    "    v = M.element()\n",
    "    guidedf = theano.function([q, v, dWt], guided(q, v, dWt))\n",
    "\n",
    "    return (guided, guidedf)\n",
    "\n",
    "# guide function\n",
    "Cholesky = T.slinalg.Cholesky()\n",
    "# phi = lambda q,v: T.tensordot(T.nlinalg.MatrixInverse()(Cholesky(M.gsharp(q))),-(q-v).flatten(),(1,0))\n",
    "phi = lambda q,v: T.tensordot(Cholesky(M.gsharp(q)).T,-(q-v).flatten(),(1,0))\n",
    "x0 = M.element()\n",
    "(Brownian_coords_guided,Brownian_coords_guidedf) = get_guided_likelihood(M,M.sde_Brownian_coords,phi,lambda x: Cholesky(M.gsharp(x)),x0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML mean estimation\n",
    "\n",
    "n_steps.set_value(50)\n",
    "\n",
    "options = {}\n",
    "options['samples_per_obs'] = 1\n",
    "options['epochs'] = 75\n",
    "options['learning_rate'] = .5e-1\n",
    "options['varphi_update_rate'] = 1.\n",
    "options['initial'] = [x+.1*np.random.randn(M.dim.eval())]\n",
    "options['verbose'] = True\n",
    "\n",
    "# Transition density\n",
    "# transition density etc.\n",
    "q0 = M.element()\n",
    "v = M.element()\n",
    "thetas = (q0,)\n",
    "log_p_Tf = theano.function([q0,v],log_p_T(q0,v,dWs(M.dim),Brownian_coords_guided,phi,options,sde=M.sde_Brownian_coords))\n",
    "dlog_p_Tf = theano.function([q0,v],dlog_p_T(thetas,q0,v,dWs(M.dim),Brownian_coords_guided,phi,options,sde=M.sde_Brownian_coords))\n",
    "p_Tf = theano.function([q0,v],T.exp(log_p_T(q0,v,dWs(M.dim),Brownian_coords_guided,phi,options,sde=M.sde_Brownian_coords)))\n",
    "\n",
    "v = x\n",
    "print(x)\n",
    "print(v)\n",
    "%time print(log_p_Tf(x,v))\n",
    "%time print(p_Tf(x,v))\n",
    "%time print(dlog_p_Tf(x,v))\n",
    "\n",
    "from src.statistics.mle import *\n",
    "\n",
    "def llog_p_T(thetas,pars):\n",
    "    (v,seed) = pars\n",
    "    if seed:\n",
    "        srng.seed(seed)\n",
    "    q = thetas[0]\n",
    "    ie = 0\n",
    "    while True:\n",
    "        try:\n",
    "            return dlog_p_Tf(q,v)\n",
    "        except ValueError: \n",
    "            if i >= 5:\n",
    "                print(ie,v)\n",
    "            ie += 1\n",
    "            pass    \n",
    "\n",
    "def update_thetas(thetas, dthetas):\n",
    "    q = thetas[0]\n",
    "    \n",
    "    q += options['learning_rate']*np.dot(M.gsharpf(q),dthetas[0]) # use Riemannian g-gradient\n",
    "    \n",
    "    return (q,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run MLE\n",
    "(thetas, log_likelihood, log_likelihoods, thetass) = iterative_mle(samples,llog_p_T,update_thetas,options)\n",
    "\n",
    "# plot\n",
    "plt.plot(range(options['epochs']),log_likelihoods)\n",
    "# plt.savefig('ML_likelihoods.pdf')\n",
    "plt.show()\n",
    "plt.plot(range(options['epochs']),thetass[0].reshape((thetass[0].shape[0],-1)))\n",
    "# plt.savefig('ML_thetas.pdf')\n",
    "plt.show()\n",
    "\n",
    "# M.newfig()\n",
    "# M.plot()\n",
    "# M.plotx(thetas[0],s=150)\n",
    "# M.plotx(np.vstack((options['initial'][0],thetass[0])),color='blue',linewidth=2.5)\n",
    "# M.plotx(np.vstack((np.zeros((1,2)),iterations)),color='red',linewidth=2.5)\n",
    "# plt.savefig('MLmean_iterations.pdf')\n",
    "# plt.show()\n",
    "\n",
    "n_steps.set_value(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SO(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SO(3)\n",
    "from src.groups.SON import *\n",
    "G = SON(3,invariance='right')\n",
    "print(G)\n",
    "\n",
    "from src.plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.group import invariant_metric\n",
    "invariant_metric.initialize(G)\n",
    "\n",
    "from src.group import energy\n",
    "energy.initialize(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.array([1e-6,0,0])\n",
    "g = G.psif(q)\n",
    "v = np.array([0,1,1])\n",
    "p = G.sharppsif(q,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sample data\n",
    "from src.stochastics import Brownian_inv\n",
    "Brownian_inv.initialize(G)\n",
    "\n",
    "G.sigma.set_value(np.diag((1.,.3,1.4))) # set metric\n",
    "K = 16 # 1024\n",
    "obss = np.zeros((K,)+g.shape)\n",
    "# srng.seed(422)\n",
    "for i in range(K):\n",
    "    (ts,gs) = G.Brownian_invf(g,dWsf(G.dim.eval()))\n",
    "    obss[i] = gs[-1]\n",
    "\n",
    "# plot samples\n",
    "newfig()\n",
    "for i in range(K):\n",
    "    G.plotg(obss[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Delyon/Hu guided process\n",
    "from src.stochastics.guided_process import *\n",
    "\n",
    "# hit target v at time t=Tend\n",
    "def get_sde_guided(sde_f, phi, sqrtCov, A=None, method='DelyonHu', integration='ito'):\n",
    "    assert (integration is 'ito' or integration is 'stratonovich')\n",
    "    assert (method is 'DelyonHu')  # more general schemes not implemented\n",
    "\n",
    "    def sde_guided(dW, t, x, log_likelihood, log_varphi, v, *ys):\n",
    "        (det, sto, X, *dys_sde) = sde_f(dW, t, x, *ys)\n",
    "        h = theano.ifelse.ifelse(T.lt(t, Tend - dt / 2),\n",
    "                                 phi(x, v) / (Tend - t),\n",
    "                                 T.zeros_like(phi(x, v))\n",
    "                                 )\n",
    "        sto = theano.ifelse.ifelse(T.lt(t, Tend - 3 * dt / 2),  # for Ito as well?\n",
    "                                   sto,\n",
    "                                   T.zeros_like(sto)\n",
    "                                   )\n",
    "\n",
    "        ### likelihood\n",
    "        dW_guided = (1 - .5 * dt / (1 - t)) * dW + dt * h  # for Ito as well?\n",
    "        sqrtCovx = sqrtCov(x)\n",
    "        Cov = dt * T.tensordot(sqrtCovx, sqrtCovx, (1, 1))\n",
    "        Pres = T.nlinalg.MatrixInverse()(Cov)\n",
    "        residual = T.tensordot(dW_guided, T.tensordot(Pres, dW_guided, (1, 0)), (0, 0))\n",
    "        log_likelihood = .5 * (-dW.shape[0] * T.log(2 * np.pi) + LogAbsDet()(Pres) - residual)\n",
    "\n",
    "#         ## correction factor\n",
    "#         ytilde = T.tensordot(X, h * (Tend - t), 1)\n",
    "#         tp1 = t + dt\n",
    "#         if integration is 'ito':\n",
    "#             xtp1 = x + dt * (det + T.tensordot(X, h, 1)) + sto\n",
    "#         elif integration is 'stratonovich':\n",
    "#             tx = x + sto\n",
    "#             xtp1 = x + dt * det + 0.5 * (sto + sde_f(dW, tp1, tx, *ys)[1])\n",
    "#         Xtp1 = sde_f(dW, tp1, xtp1, *ys)[2]\n",
    "#         ytildetp1 = T.tensordot(Xtp1, phi(xtp1, v), 1)\n",
    "\n",
    "#         # set default A if not specified\n",
    "#         Af = A if A is not None else lambda x, v, w: T.tensordot(v, T.tensordot(T.nlinalg.MatrixInverse()(T.tensordot(X, X, (1, 1))), w, 1), 1)\n",
    "\n",
    "#         #     add t1 term for general phi\n",
    "#         #     dxbdxt = theano.gradient.Rop((Gx-x[0]).flatten(),x[0],dx[0]) # use this for general phi\n",
    "#         t2 = theano.ifelse.ifelse(T.lt(t, Tend - dt / 2),\n",
    "#                                   -Af(x, ytilde, dt * det) / (Tend - t),\n",
    "#                                   # check det term for Stratonovich (correction likely missing)\n",
    "#                                   0.)\n",
    "#         t34 = theano.ifelse.ifelse(T.lt(tp1, Tend - dt / 2),\n",
    "#                                    -(Af(xtp1, ytildetp1, ytildetp1) - Af(x, ytildetp1, ytildetp1)) / (\n",
    "#                                    2 * (Tend - tp1 + dt * T.gt(tp1, Tend - dt / 2))),\n",
    "#                                    # last term in divison is to avoid NaN with non-lazy Theano conditional evaluation\n",
    "#                                    0.)\n",
    "        log_varphi = log_likelihood#t2 + t34\n",
    "\n",
    "        return (det + T.tensordot(X, h, 1), sto, X, log_likelihood, log_varphi, T.zeros_like(v), *dys_sde)\n",
    "\n",
    "    return sde_guided\n",
    "\n",
    "def get_guided_likelihood(M, sde_f, phi, sqrtCov, q, A=None, method='DelyonHu', integration='ito'):\n",
    "    sde_guided = get_sde_guided(sde_f, phi, sqrtCov, A, method, integration)\n",
    "    guided = lambda q, v, dWt: integrate_sde(sde_guided,\n",
    "                                             integrator_ito if method is 'ito' else integrator_stratonovich,\n",
    "                                             q, dWt, T.constant(0.), T.constant(0.), v)\n",
    "    v = M.element()\n",
    "    guidedf = theano.function([q, v, dWt], guided(q, v, dWt)[:4])\n",
    "\n",
    "    return (guided, guidedf)\n",
    "\n",
    "# parameters\n",
    "g0 = G.element()\n",
    "thetas = (g0, G.sigma,)\n",
    "thetas_true = [g]+[theta.eval() for theta in thetas[1:]]\n",
    "\n",
    "# guide function\n",
    "phi = lambda g,v: G.LAtoV(G.invtrns(G.inv(g),v)-G.e)\n",
    "# phi = lambda g,v: T.tensordot(G.inv(G.sigma),G.LAtoV(G.log(G.invtrns(G.inv(g),v))),(1,0))\n",
    "\n",
    "# (Brownian_inv_guided,Brownian_inv_guidedf) = get_guided_likelihood(G,G.sde_Brownian_inv,phi,lambda g: G.sigma, g0, thetas, A=G.gG, integration='stratonovich')\n",
    "(Brownian_inv_guided,Brownian_inv_guidedf) = get_guided_likelihood(G,G.sde_Brownian_inv,phi,lambda g: G.sigma, g0, A=G.gG, integration='stratonovich')\n",
    "\n",
    "w = G.psif(v)\n",
    "(ts,gs,log_likelihood,log_varphi) = Brownian_inv_guidedf(g,w,dWsf(G.dim.eval()))[:4]\n",
    "print(\"log likelihood: \", log_likelihood[-1], \", log varphi: \", log_varphi[-1])\n",
    "\n",
    "newfig()\n",
    "G.plotg(gs)\n",
    "G.plotg(w,color='k')\n",
    "# plt.savefig('/home/stefan/Dropbox/projects/diffusion/figures/SO3-bridge.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {}\n",
    "options['samples_per_obs'] = 20\n",
    "options['epochs'] = 10\n",
    "options['learning_rate'] = 6.e-2\n",
    "options['varphi_update_rate'] = 1.\n",
    "options['verbose'] = True\n",
    "options['initial'] = [obss[0], # random value\n",
    "                      np.diag((.08,.1,.2)),]\n",
    "# options['update_v'] = lambda g: theano.gradient.disconnected_grad(Brownian_inv_fiber(g,dWs(G.dim))[1][-1])\n",
    "# options['update_vf'] = lambda g: Brownian_inv_fiberf(g,dWsf(G.dim.eval()))[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# helper for log-transition density\n",
    "def p_T_log_p_T(g, v, dWs, bridge_sde, phi, options, sigma=None, sde=None):\n",
    "    \"\"\" Monte Carlo approximation of log transition density from guided process \"\"\"\n",
    "    if sigma is None and sde is not None:\n",
    "        (_, _, XT) = sde(dWs, Tend, v)  # starting point of SDE, we need diffusion field X at t=0\n",
    "        sigma = XT\n",
    "    assert (sigma is not None)\n",
    "    \n",
    "    # sample noise\n",
    "    (cout, updates) = theano.scan(fn=lambda x: dWs,\n",
    "                                  outputs_info=[T.zeros_like(dWs)],\n",
    "                                  n_steps=options['samples_per_obs'])\n",
    "    dWsi = cout\n",
    "\n",
    "    if not 'update_v' in options:\n",
    "        # v constant throughout sampling\n",
    "        print(\"transition density with v constant\")\n",
    "\n",
    "        # bridges\n",
    "        Cgv = T.sum(phi(g, v) ** 2)\n",
    "        def bridge_logvarphis(dWs, log_varphi):\n",
    "            (ts, gs, log_likelihood, log_varphi, _) = bridge_sde(g, v, dWs)\n",
    "            return log_varphi[-1]\n",
    "\n",
    "        (cout, updates) = theano.scan(fn=bridge_logvarphis,\n",
    "                                      outputs_info=[T.constant(0.)],\n",
    "                                      sequences=[dWsi])\n",
    "        log_varphi = T.log(T.mean(T.exp(cout)))\n",
    "        log_p_T = -.5 * sigma.shape[0] * T.log(2. * np.pi * Tend) - LogAbsDet()(sigma) - Cgv / (2. * Tend) + log_varphi\n",
    "        p_T = T.exp(log_p_T)\n",
    "    else:\n",
    "        # update v during sampling, e.g. for fiber densities\n",
    "        print(\"transition density with v updates\")\n",
    "\n",
    "        # bridges\n",
    "        Cgv = T.sum(phi(g, v) ** 2)\n",
    "        def bridge_p_T(dWs, lp_T, lv):\n",
    "#             Cgv = T.sum(phi(g, lv) ** 2)\n",
    "            (ts, gs, log_likelihood, log_varphi, _) = bridge_sde(g, v, dWs)            \n",
    "            lp_T =  T.power(2.*np.pi*Tend,-.5*sigma.shape[0])/T.abs_(T.nlinalg.Det()(sigma))*T.exp(-Cgv/(2.*Tend))*T.exp(log_varphi[-1])\n",
    "#             lv = options['update_v'](lv)                        \n",
    "            return (lp_T, lv)\n",
    "\n",
    "        (cout, updates) = theano.scan(fn=bridge_p_T,\n",
    "                                      outputs_info=[T.constant(0.), v],\n",
    "                                      sequences=[dWsi])\n",
    "        p_T = T.mean(cout[:][0])\n",
    "        log_p_T = T.log(p_T)\n",
    "        v = cout[-1][1]\n",
    "    \n",
    "    return (p_T,log_p_T,v)\n",
    "\n",
    "def p_T(*args,**kwargs): return p_T_log_p_T(*args,**kwargs)[0]\n",
    "def log_p_T(*args,**kwargs): return p_T_log_p_T(*args,**kwargs)[1]\n",
    "\n",
    "def dp_T(thetas,*args,**kwargs):\n",
    "    \"\"\" Monte Carlo approximation of transition density gradient \"\"\"\n",
    "    lp_T = p_T(*args,**kwargs)\n",
    "    return (lp_T,)+tuple(T.grad(lp_T,theta) for theta in thetas)\n",
    "\n",
    "def dlog_p_T(thetas,*args,**kwargs):\n",
    "    \"\"\" Monte Carlo approximation of log transition density gradient \"\"\"\n",
    "    llog_p_T = log_p_T(*args,**kwargs)\n",
    "    return (llog_p_T,)+tuple(T.grad(llog_p_T,theta) for theta in thetas)\n",
    "\n",
    "\n",
    "# Transition density\n",
    "v0 = G.element()\n",
    "p_Tf = theano.function([g0,v0],p_T(g0,v0,dWs(G.dim),Brownian_inv_guided,phi,options,sde=G.sde_Brownian_inv,sigma=G.sigma))\n",
    "log_p_Tf = theano.function([g0,v0],log_p_T(g0,v0,dWs(G.dim),Brownian_inv_guided,phi,options,sde=G.sde_Brownian_inv,sigma=G.sigma))\n",
    "dlog_p_Tf = theano.function([g0,v0],dlog_p_T(thetas,g0,v0,dWs(G.dim),Brownian_inv_guided,phi,options,sde=G.sde_Brownian_inv,sigma=G.sigma))\n",
    "\n",
    "# G.sigma.set_value(np.diag((1.,.3,1.6))) # set metric\n",
    "\n",
    "# # on G\n",
    "print(p_Tf(g,G.psif(v))) \n",
    "print(log_p_Tf(g,G.psif(v))) \n",
    "print(dlog_p_Tf(g,G.psif(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(p_Tf(g,G.psif(v))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(dlog_p_Tf(g,G.psif(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "L = options['samples_per_obs']\n",
    "gsl = np.zeros((L,)+g.shape)\n",
    "vl = G.psif(v)\n",
    "for l in range(L):\n",
    "#     (ts,gs) = Brownian_inv_fiberf(vl,dWsf(G.dim.eval()))\n",
    "#     (ts,gs) = G.Brownian_invf(vl,dWsf(G.dim.eval()))\n",
    "    (ts,gs,_,_) = Brownian_inv_guidedf(vl,w,dWsf(G.dim.eval()))\n",
    "    gsl[l] = gs[-1]\n",
    "    vl = gs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# samples for MLE\n",
    "# G.sigma.set_value(1.*np.eye(G.dim.eval())) # set metric, uniform\n",
    "G.sigma.set_value(np.diag((.2,.2,1.5))) # anisotropic\n",
    "K = 16\n",
    "n_steps.set_value(20)\n",
    "\n",
    "obss = np.zeros((K,)+g.shape)\n",
    "# srng.seed(422)\n",
    "for i in range(K):\n",
    "    (ts,gs) = G.Brownian_invf(g,dWsf(G.dim.eval()))\n",
    "    obss[i] = gs[-1]\n",
    "\n",
    "# plot samples\n",
    "newfig()\n",
    "for i in range(K):\n",
    "    G.plotg(obss[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# on SO(3)\n",
    "from src.statistics.mle import *\n",
    "\n",
    "def llog_p_T(thetas,pars):\n",
    "    (v,seed) = pars\n",
    "    if seed:\n",
    "        srng.seed(seed)\n",
    "    g = thetas[0]\n",
    "    G.sigma.set_value(thetas[1])\n",
    "    return dlog_p_Tf(g,v)\n",
    "\n",
    "def update_thetas(thetas, dthetas):\n",
    "    g = thetas[0]\n",
    "    sigma = thetas[1]\n",
    "    \n",
    "#     g = G.to_groupf(g+options['learning_rate']*dthetas[0])\n",
    "    sigma += options['learning_rate']*dthetas[1]\n",
    "    \n",
    "    return (g,sigma)\n",
    "\n",
    "# run MLE\n",
    "(thetas, log_likelihood, log_likelihoods, thetass) = iterative_mle(obss,llog_p_T,update_thetas,options)\n",
    "\n",
    "# plot\n",
    "plt.plot(range(options['epochs'])[2:],log_likelihoods[2:])\n",
    "plt.show()\n",
    "plt.plot(range(options['epochs']),thetass[0].reshape((thetass[0].shape[0],-1)))\n",
    "plt.hlines(thetas_true[0].flatten(),plt.xlim()[0],plt.xlim()[1],color='r')\n",
    "plt.show()\n",
    "plt.plot(range(options['epochs']),thetass[1].reshape((thetass[1].shape[0],-1)))\n",
    "plt.hlines(thetas_true[1].flatten(),plt.xlim()[0],plt.xlim()[1],color='r')\n",
    "plt.show()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample bridges\n",
    "# def lbridge_sampling(thetas,*args,**kwargs):\n",
    "#     g = thetas[0]\n",
    "#     G.sigma.set_value(thetas[1])\n",
    "#     return partial(bridge_sampling,g,Brownian_inv_guidedf,lambda: dWsf(G.dim.eval()),options)(*args,**kwargs)\n",
    "\n",
    "# log_phis = np.zeros((K,))\n",
    "# try:\n",
    "#     mpu.openPool()\n",
    "#     sol = mpu.pool.imap(partial(lbridge_sampling,options['initial']),mpu.inputArgs(obss,np.random.randint(1000,size=K)))\n",
    "#     res = list(sol)\n",
    "#     bridges = mpu.getRes(res,0)\n",
    "#     log_varphis = mpu.getRes(res,1)\n",
    "#     log_likelihoods = mpu.getRes(res,2)\n",
    "# except:\n",
    "#     mpu.closePool()\n",
    "#     raise\n",
    "# else:\n",
    "#     mpu.closePool()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
